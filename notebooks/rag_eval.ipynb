{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U langchain langchain-community langchain-chroma langchain-openai faiss-cpu\n",
    "%pip install -q -U langchain_experimental lastmile-eval \"lastmile-eval[ui]\"\n",
    "%pip install -q -U python-dotenv\n",
    "%pip install \"tracing-auto-instrumentation[langchain]\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader \n",
    "from langchain_community.document_loaders.text import TextLoader \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.vectorstores import Chroma \n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# from tracing_auto_instrumentation.langchain import LangChainInstrumentor\n",
    "\n",
    "# # Create an instance of LangChainInstrumentor and instrument the code\n",
    "# instrumentor = LangChainInstrumentor(project_name=\"RAG Eval Test\")\n",
    "# instrumentor.instrument()\n",
    "\n",
    "print(\"Loading documents...\")\n",
    "knowledgeDirectoryPath = \"knowledge/structured\"\n",
    "loader = DirectoryLoader(knowledgeDirectoryPath, glob=\"**/*.*\", loader_cls=TextLoader) \n",
    "loaded_docs = loader.load() \n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\") \n",
    "\n",
    "print(\"Splitting documents...\")\n",
    "#splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100) \n",
    "splitter = SemanticChunker(embeddings)\n",
    "chunks = splitter.split_documents(loaded_docs) \n",
    "\n",
    "print(\"Creating embeddings...\")\n",
    "vector_store = Chroma.from_documents(chunks, embeddings) \n",
    "\n",
    "print(\"Done loading documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(query):\n",
    "    # Get documents similar to the query ith their scores\n",
    "    docs_and_score = vector_store.similarity_search_with_score(query, k=5)\n",
    "    \n",
    "    # Remove duplicates and unrelated documents\n",
    "    unique_docs = []\n",
    "    unique_docs_and_score = []\n",
    "    seen = set()\n",
    "    ordered_byScore = sorted(docs_and_score, key=lambda x: x[1])\n",
    "    for doc in ordered_byScore:\n",
    "        content = doc[0].page_content\n",
    "        score = doc[1]\n",
    "        if score < 1 and content not in seen:\n",
    "            unique_docs.append(content)\n",
    "            unique_docs_and_score.append(doc)\n",
    "            seen.add(content)\n",
    "            \n",
    "    context = \"\\n\\n\".join(unique_docs)\n",
    "    \n",
    "    return context, unique_docs_and_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send query to retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context, results = retrieve_docs(\"I'm a new associate. What should I do to be successful?\")\n",
    "# context, results = retrieve_docs(\"What is the capital of Argentina?\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send query to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from lastmile_eval.rag.debugger.tracing import get_lastmile_tracer\n",
    "from lastmile_eval.rag.debugger.common.types import RagFlowType\n",
    "from lastmile_eval.rag.debugger.api import LastMileTracer\n",
    "\n",
    "SCOPED_PROMPT = \"\"\"\n",
    "If the answer to the user's question is not contained in the provided context, answer ðŸ¤·.\n",
    "\"\"\"\n",
    "# If the answer to the user question is not contained in the provided context and cannot be inferred from it, \n",
    "# answer ðŸ¤·.\n",
    "# \"\"\"\n",
    "\n",
    "USE_MARKDOWN_PROMPT = \"\"\"\n",
    "Use Markdown to format your response.\n",
    "\"\"\"\n",
    "\n",
    "TRANSPARENT_CONTEXT = \"\"\"\n",
    "Do not mention the context in your answer.\n",
    "\"\"\"\n",
    "\n",
    "PROJECT_NAME = \"RAG Test 4\"\n",
    "\n",
    "# Instantiate LastMile Tracer object\n",
    "tracer: LastMileTracer = get_lastmile_tracer(\n",
    "    tracer_name=\"my-tracer\",\n",
    "    project_name=PROJECT_NAME,\n",
    "    rag_flow_type=RagFlowType.QUERY,\n",
    ")\n",
    "\n",
    "tracer: LastMileTracer = get_lastmile_tracer(\"My-Project\")\n",
    "\n",
    "def get_prompt(instructions, scoped_answer, use_markdown, context):\n",
    "    content = instructions \\\n",
    "        + TRANSPARENT_CONTEXT \\\n",
    "        + (SCOPED_PROMPT if scoped_answer else \"\") \\\n",
    "        + (USE_MARKDOWN_PROMPT if use_markdown else \"\") \\\n",
    "        + \"\\nContext:\\n\" + context \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessage(content=content),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "@tracer.trace_function()\n",
    "def get_response(query, modelfamily, model, instructions, scoped_answer, use_markdown, temperature):    \n",
    "    client = get_client(modelfamily, model, temperature)\n",
    "    context, docs_with_scores = retrieve_docs(query)\n",
    "    prompt = get_prompt(instructions, scoped_answer, use_markdown, context)\n",
    "\n",
    "    chain = prompt | client\n",
    "    response = chain.invoke({\"messages\": [HumanMessage(content=query)]})\n",
    "    \n",
    "    metadata = get_metadata(modelfamily, model, response)\n",
    "    \n",
    "    # Log query event to the trace\n",
    "    tracer.add_query_event(\n",
    "        query=query,\n",
    "        llm_output=response.content,\n",
    "        system_prompt=\"system prompt\",\n",
    "        metadata={\"llm_name\": model, \"temperature\": temperature, \"model_family\": modelfamily},\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"content\": response.content,\n",
    "        \"docs_with_scores\": docs_with_scores,\n",
    "        \"metadata\": metadata,\n",
    "        \"prompt\": prompt,\n",
    "        }\n",
    "    \n",
    "def get_client(model_family, model, temperature):\n",
    "    if model_family == \"openai\":\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        model = ChatOpenAI(model=model, temperature=temperature)\n",
    "    elif model_family == \"anthropic\":\n",
    "        from langchain_anthropic import ChatAnthropic\n",
    "        model = ChatAnthropic(model=model, temperature=temperature)\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_family} not recognized\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_metadata(model_family, model, response):\n",
    "    if model_family == \"anthropic\":\n",
    "        usage = {k: v for k, v in response.response_metadata[\"usage\"].items() if k in (\"input_tokens\", \"output_tokens\")}\n",
    "    elif model_family == \"openai\":\n",
    "        usage = {k: v for k, v in response.usage_metadata.items() if k in (\"input_tokens\", \"output_tokens\")}\n",
    "    else:\n",
    "        raise ValueError(f\"Model family {model_family} not recognized\")\n",
    "    \n",
    "    total_tokens = usage[\"input_tokens\"] + usage[\"output_tokens\"]\n",
    "\n",
    "    return {\"model\": model, **usage, \"total_tokens\": total_tokens}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_response(\n",
    "    \"I'm a new associate. What should I do to be successful?\",                  # query\n",
    "    \"openai\",                                                                   # Model family\n",
    "    \"gpt-3.5-turbo\",                                                            # Model\n",
    "    \"\",                                                                         # Instructions       \n",
    "    True,                                                                       # Scoped answer    \n",
    "    True,                                                                       # Use markdown\n",
    "    0                                                                           # Temperature  \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
